{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import xlrd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n"
     ]
    }
   ],
   "source": [
    "data = xlrd.open_workbook('data/fire_theft.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet = data.sheet_by_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray([sheet.row_values(i) for i in range(1, sheet.nrows)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = sheet.nrows - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32,name='X')\n",
    "Y = tf.placeholder(dtype=tf.float32,name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(0.0, name='weights',dtype=tf.float32)\n",
    "b = tf.Variable(0.0, name='bias',dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = X*w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.square(y_predicted - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:0 loss:8940.417485871352 \n",
      "iteration:1 loss:10209.569467804395 \n",
      "iteration:2 loss:9674.196445380338 \n",
      "iteration:3 loss:9166.89653567411 \n",
      "iteration:4 loss:8686.193675940856 \n",
      "iteration:5 loss:8230.702799132094 \n",
      "iteration:6 loss:7799.098617656622 \n",
      "iteration:7 loss:7390.122302057687 \n",
      "iteration:8 loss:7002.59626738634 \n",
      "iteration:9 loss:6635.389248430263 \n",
      "iteration:10 loss:6287.436095390469 \n",
      "iteration:11 loss:5957.732596684014 \n",
      "iteration:12 loss:5645.3160440465435 \n",
      "iteration:13 loss:5349.2848936163355 \n",
      "iteration:14 loss:5068.77538066078 \n",
      "iteration:15 loss:4802.975731068524 \n",
      "iteration:16 loss:4551.114624809474 \n",
      "iteration:17 loss:4312.460411467822 \n",
      "iteration:18 loss:4086.3203499803785 \n",
      "iteration:19 loss:3872.037873654859 \n",
      "iteration:20 loss:3668.9936809772626 \n",
      "iteration:21 loss:3476.597346818773 \n",
      "iteration:22 loss:3294.290605078917 \n",
      "iteration:23 loss:3121.5432054772973 \n",
      "iteration:24 loss:2957.8537081636023 \n",
      "iteration:25 loss:2802.7476469328394 \n",
      "iteration:26 loss:2655.7753857084317 \n",
      "iteration:27 loss:2516.5106603081804 \n",
      "iteration:28 loss:2384.547965424368 \n",
      "iteration:29 loss:2259.5054152054945 \n",
      "iteration:30 loss:2141.019392299466 \n",
      "iteration:31 loss:2028.7468142081052 \n",
      "iteration:32 loss:1922.362236584071 \n",
      "iteration:33 loss:1821.5579435551772 \n",
      "iteration:34 loss:1726.0379126126645 \n",
      "iteration:35 loss:1635.527158419718 \n",
      "iteration:36 loss:1549.7618344631046 \n",
      "iteration:37 loss:1468.4938156652497 \n",
      "iteration:38 loss:1391.4878233530326 \n",
      "iteration:39 loss:1318.5191416056477 \n",
      "iteration:40 loss:1249.3775928203831 \n",
      "iteration:41 loss:1183.861449661199 \n",
      "iteration:42 loss:1121.7813274331857 \n",
      "iteration:43 loss:1062.9571949350066 \n",
      "iteration:44 loss:1007.2169499144075 \n",
      "iteration:45 loss:954.4009504162241 \n",
      "iteration:46 loss:904.3531096305232 \n",
      "iteration:47 loss:856.9294192141388 \n",
      "iteration:48 loss:811.9926125366474 \n",
      "iteration:49 loss:769.4124928173842 \n",
      "iteration:50 loss:729.0659313760698 \n",
      "iteration:51 loss:690.8339058874408 \n",
      "iteration:52 loss:654.6072661478538 \n",
      "iteration:53 loss:620.2805880836677 \n",
      "iteration:54 loss:587.7541012704605 \n",
      "iteration:55 loss:556.9334165143373 \n",
      "iteration:56 loss:527.729167149635 \n",
      "iteration:57 loss:500.05502405110747 \n",
      "iteration:58 loss:473.83204689709237 \n",
      "iteration:59 loss:448.9853368490294 \n",
      "iteration:60 loss:425.4411246108939 \n",
      "iteration:61 loss:403.13204852247145 \n",
      "iteration:62 loss:381.9923155235592 \n",
      "iteration:63 loss:361.9609869538108 \n",
      "iteration:64 loss:342.98076679193764 \n",
      "iteration:65 loss:324.99547367193736 \n",
      "iteration:66 loss:307.9532528735872 \n",
      "iteration:67 loss:291.8039972381084 \n",
      "iteration:68 loss:276.5021121882164 \n",
      "iteration:69 loss:262.003953506588 \n",
      "iteration:70 loss:248.26579649100313 \n",
      "iteration:71 loss:235.24739230841806 \n",
      "iteration:72 loss:222.911592395787 \n",
      "iteration:73 loss:211.2216411899717 \n",
      "iteration:74 loss:200.14532118593343 \n",
      "iteration:75 loss:189.64950969866186 \n",
      "iteration:76 loss:179.70399682904826 \n",
      "iteration:77 loss:170.2804601287935 \n",
      "iteration:78 loss:161.35154564437107 \n",
      "iteration:79 loss:152.89087187526457 \n",
      "iteration:80 loss:144.87409601955733 \n",
      "iteration:81 loss:137.27722509646264 \n",
      "iteration:82 loss:130.07840935625427 \n",
      "iteration:83 loss:123.2576671512943 \n",
      "iteration:84 loss:116.79441671469249 \n",
      "iteration:85 loss:110.66990542577696 \n",
      "iteration:86 loss:104.86638093357033 \n",
      "iteration:87 loss:99.36675232947164 \n",
      "iteration:88 loss:94.15566698007751 \n",
      "iteration:89 loss:89.2180576649298 \n",
      "iteration:90 loss:84.53926611279894 \n",
      "iteration:91 loss:80.10607777150653 \n",
      "iteration:92 loss:75.90583176730433 \n",
      "iteration:93 loss:71.92607159103864 \n",
      "iteration:94 loss:68.15449338500912 \n",
      "iteration:95 loss:64.5804513672083 \n",
      "iteration:96 loss:61.19392117130701 \n",
      "iteration:97 loss:57.98503181200067 \n",
      "iteration:98 loss:54.944405893682415 \n",
      "iteration:99 loss:52.06358508450285 \n",
      "iteration:100 loss:49.33366070978809 \n",
      "iteration:101 loss:46.746846502286644 \n",
      "iteration:102 loss:44.295749684566545 \n",
      "iteration:103 loss:41.972925164362096 \n",
      "iteration:104 loss:39.77206111804844 \n",
      "iteration:105 loss:37.686672792307945 \n",
      "iteration:106 loss:35.710482978265645 \n",
      "iteration:107 loss:33.83781456493671 \n",
      "iteration:108 loss:32.06328400240818 \n",
      "iteration:109 loss:30.381793396387366 \n",
      "iteration:110 loss:28.78875219726251 \n",
      "iteration:111 loss:27.279344556423894 \n",
      "iteration:112 loss:25.848591155096074 \n",
      "iteration:113 loss:24.49316929854467 \n",
      "iteration:114 loss:23.208609437839186 \n",
      "iteration:115 loss:21.991602603917272 \n",
      "iteration:116 loss:20.83859171302538 \n",
      "iteration:117 loss:19.746046919524815 \n",
      "iteration:118 loss:18.71033679490938 \n",
      "iteration:119 loss:17.729020918868628 \n",
      "iteration:120 loss:16.799515698432515 \n",
      "iteration:121 loss:15.918685812128388 \n",
      "iteration:122 loss:15.084032518221647 \n",
      "iteration:123 loss:14.293059320934844 \n",
      "iteration:124 loss:13.54359053947337 \n",
      "iteration:125 loss:12.833346979201451 \n",
      "iteration:126 loss:12.160473627220199 \n",
      "iteration:127 loss:11.522758126742701 \n",
      "iteration:128 loss:10.918615809529001 \n",
      "iteration:129 loss:10.346185500275169 \n",
      "iteration:130 loss:9.803758496422233 \n",
      "iteration:131 loss:9.289725496761093 \n",
      "iteration:132 loss:8.802416645794437 \n",
      "iteration:133 loss:8.340908391641278 \n",
      "iteration:134 loss:7.903762902788003 \n",
      "iteration:135 loss:7.489269245081232 \n",
      "iteration:136 loss:7.096507103433396 \n",
      "iteration:137 loss:6.724417550376529 \n",
      "iteration:138 loss:6.371830375552236 \n",
      "iteration:139 loss:6.03782539194799 \n",
      "iteration:140 loss:5.721273181803554 \n",
      "iteration:141 loss:5.421187789328542 \n",
      "iteration:142 loss:5.136951386597502 \n",
      "iteration:143 loss:4.867424512256548 \n",
      "iteration:144 loss:4.612243493542337 \n",
      "iteration:145 loss:4.370414129301935 \n",
      "iteration:146 loss:4.14128374971915 \n",
      "iteration:147 loss:3.924035735675716 \n",
      "iteration:148 loss:3.7182200207316782 \n",
      "iteration:149 loss:3.5232131963603024 \n",
      "iteration:150 loss:3.3384237237660273 \n",
      "iteration:151 loss:3.163438025254436 \n",
      "iteration:152 loss:2.9975950518855825 \n",
      "iteration:153 loss:2.840372689624928 \n",
      "iteration:154 loss:2.691438251262298 \n",
      "iteration:155 loss:2.550295363256737 \n",
      "iteration:156 loss:2.4165542839946283 \n",
      "iteration:157 loss:2.289788394165953 \n",
      "iteration:158 loss:2.1697484043907025 \n",
      "iteration:159 loss:2.0559458994575834 \n",
      "iteration:160 loss:1.9481266074508312 \n",
      "iteration:161 loss:1.846038331717864 \n",
      "iteration:162 loss:1.7491738655990048 \n",
      "iteration:163 loss:1.6574322395208583 \n",
      "iteration:164 loss:1.5705196120034088 \n",
      "iteration:165 loss:1.4881969383277465 \n",
      "iteration:166 loss:1.4101403955719434 \n",
      "iteration:167 loss:1.3362031623291841 \n",
      "iteration:168 loss:1.2661262471410737 \n",
      "iteration:169 loss:1.19969201542699 \n",
      "iteration:170 loss:1.136749778423109 \n",
      "iteration:171 loss:1.077144012280769 \n",
      "iteration:172 loss:1.020655138509028 \n",
      "iteration:173 loss:0.9671302050483064 \n",
      "iteration:174 loss:0.9163863310714078 \n",
      "iteration:175 loss:0.86831135655666 \n",
      "iteration:176 loss:0.8227364764388767 \n",
      "iteration:177 loss:0.7796126633656968 \n",
      "iteration:178 loss:0.7387219552038005 \n",
      "iteration:179 loss:0.6999675996557926 \n",
      "iteration:180 loss:0.6632403636649542 \n",
      "iteration:181 loss:0.6284821150075004 \n",
      "iteration:182 loss:0.5955114527059777 \n",
      "iteration:183 loss:0.5643158873208449 \n",
      "iteration:184 loss:0.534713191951596 \n",
      "iteration:185 loss:0.5066867133791675 \n",
      "iteration:186 loss:0.48016633225415717 \n",
      "iteration:187 loss:0.4549581343126192 \n",
      "iteration:188 loss:0.4310764545589336 \n",
      "iteration:189 loss:0.4084744678621064 \n",
      "iteration:190 loss:0.38703118900957634 \n",
      "iteration:191 loss:0.36676073486887617 \n",
      "iteration:192 loss:0.34752716971343034 \n",
      "iteration:193 loss:0.32932407545740716 \n",
      "iteration:194 loss:0.3120899909408763 \n",
      "iteration:195 loss:0.2957297951397777 \n",
      "iteration:196 loss:0.280250879466621 \n",
      "iteration:197 loss:0.26556796709337505 \n",
      "iteration:198 loss:0.2516434024037153 \n",
      "iteration:199 loss:0.23847533151638345 \n",
      "iteration:200 loss:0.2260127137269592 \n",
      "iteration:201 loss:0.21416277088064817 \n",
      "iteration:202 loss:0.2029485295570339 \n",
      "iteration:203 loss:0.1923189874723903 \n",
      "iteration:204 loss:0.18221205237932736 \n",
      "iteration:205 loss:0.17263141733928933 \n",
      "iteration:206 loss:0.16358366110216593 \n",
      "iteration:207 loss:0.1550080887209333 \n",
      "iteration:208 loss:0.14689336319133872 \n",
      "iteration:209 loss:0.13917916582431644 \n",
      "iteration:210 loss:0.13187995787302498 \n",
      "iteration:211 loss:0.12496368998472462 \n",
      "iteration:212 loss:0.11842358248395612 \n",
      "iteration:213 loss:0.11221022595054819 \n",
      "iteration:214 loss:0.10633721076737856 \n",
      "iteration:215 loss:0.10077571433066623 \n",
      "iteration:216 loss:0.09549732855157345 \n",
      "iteration:217 loss:0.09047642455698224 \n",
      "iteration:218 loss:0.08573242361671873 \n",
      "iteration:219 loss:0.08123252621953725 \n",
      "iteration:220 loss:0.07697573450786876 \n",
      "iteration:221 loss:0.07294270786951529 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:222 loss:0.06911603545086109 \n",
      "iteration:223 loss:0.0654948536066513 \n",
      "iteration:224 loss:0.062053244346316205 \n",
      "iteration:225 loss:0.05879886966795311 \n",
      "iteration:226 loss:0.05572248551106895 \n",
      "iteration:227 loss:0.052813695881923195 \n",
      "iteration:228 loss:0.05005560467179748 \n",
      "iteration:229 loss:0.0474163430299086 \n",
      "iteration:230 loss:0.044917466715560295 \n",
      "iteration:231 loss:0.04256053174685803 \n",
      "iteration:232 loss:0.0403149530684459 \n",
      "iteration:233 loss:0.038190269136975985 \n",
      "iteration:234 loss:0.036195833283272805 \n",
      "iteration:235 loss:0.03429989138385281 \n",
      "iteration:236 loss:0.03250761748495279 \n",
      "iteration:237 loss:0.03080195520669804 \n",
      "iteration:238 loss:0.02918717656939407 \n",
      "iteration:239 loss:0.027653419954731362 \n",
      "iteration:240 loss:0.026199159874522593 \n",
      "iteration:241 loss:0.024819345690048067 \n",
      "iteration:242 loss:0.0235147744169808 \n",
      "iteration:243 loss:0.02227783077978529 \n",
      "iteration:244 loss:0.021109634959429968 \n",
      "iteration:245 loss:0.020004758462164318 \n",
      "iteration:246 loss:0.018956634543428663 \n",
      "iteration:247 loss:0.01796567406563554 \n",
      "iteration:248 loss:0.017022279942466412 \n",
      "iteration:249 loss:0.016124856942042243 \n",
      "iteration:250 loss:0.015281490093911998 \n",
      "iteration:251 loss:0.01447687174731982 \n",
      "iteration:252 loss:0.013719480237341486 \n",
      "iteration:253 loss:0.013000237740925513 \n",
      "iteration:254 loss:0.012315571704675676 \n",
      "iteration:255 loss:0.011673325934680179 \n",
      "iteration:256 loss:0.01106510364843416 \n",
      "iteration:257 loss:0.01048806444669026 \n",
      "iteration:258 loss:0.009935976075212238 \n",
      "iteration:259 loss:0.009414972460945137 \n",
      "iteration:260 loss:0.008922078766772756 \n",
      "iteration:261 loss:0.008456900046439841 \n",
      "iteration:262 loss:0.008014993927645264 \n",
      "iteration:263 loss:0.007591803143441211 \n",
      "iteration:264 loss:0.007194293662905693 \n",
      "iteration:265 loss:0.006822291325079277 \n",
      "iteration:266 loss:0.006461709486757172 \n",
      "iteration:267 loss:0.006120628742792178 \n",
      "iteration:268 loss:0.0057983391925517935 \n",
      "iteration:269 loss:0.005494203309353907 \n",
      "iteration:270 loss:0.005206538327911403 \n",
      "iteration:271 loss:0.004933532189170364 \n",
      "iteration:272 loss:0.0046735688047192525 \n",
      "iteration:273 loss:0.00442631050827913 \n",
      "iteration:274 loss:0.004192786098428769 \n",
      "iteration:275 loss:0.003973766535636969 \n",
      "iteration:276 loss:0.003768366757867625 \n",
      "iteration:277 loss:0.0035701034430530854 \n",
      "iteration:278 loss:0.003382616036105901 \n",
      "iteration:279 loss:0.0032040305814007297 \n",
      "iteration:280 loss:0.0030342154313984793 \n",
      "iteration:281 loss:0.002872891924198484 \n",
      "iteration:282 loss:0.0027201427801628597 \n",
      "iteration:283 loss:0.002577086510427762 \n",
      "iteration:284 loss:0.0024411113481619395 \n",
      "iteration:285 loss:0.0023137785465223715 \n",
      "iteration:286 loss:0.002192805401136866 \n",
      "iteration:287 loss:0.0020792499235540163 \n",
      "iteration:288 loss:0.0019716399401659146 \n",
      "iteration:289 loss:0.0018675612045626622 \n",
      "iteration:290 loss:0.0017707512233755551 \n",
      "iteration:291 loss:0.0016804334118205588 \n",
      "iteration:292 loss:0.0015949648914102 \n",
      "iteration:293 loss:0.0015120851167012006 \n",
      "iteration:294 loss:0.0014331177771964576 \n",
      "iteration:295 loss:0.0013584947082563303 \n",
      "iteration:296 loss:0.00128874773872667 \n",
      "iteration:297 loss:0.0012209556261950638 \n",
      "iteration:298 loss:0.001156089852884179 \n",
      "iteration:299 loss:0.001094627474230947 \n",
      "iteration:300 loss:0.0010355922240705695 \n",
      "iteration:301 loss:0.0009806074194784742 \n",
      "iteration:302 loss:0.000928851655771723 \n",
      "iteration:303 loss:0.0008794785462669097 \n",
      "iteration:304 loss:0.000832275502034463 \n",
      "iteration:305 loss:0.0007879900877014734 \n",
      "iteration:306 loss:0.0007468557232641615 \n",
      "iteration:307 loss:0.0007067433871270623 \n",
      "iteration:308 loss:0.0006694129442621488 \n",
      "iteration:309 loss:0.0006351782867568545 \n",
      "iteration:310 loss:0.0006025088732712902 \n",
      "iteration:311 loss:0.0005709648030460812 \n",
      "iteration:312 loss:0.0005413497165136505 \n",
      "iteration:313 loss:0.0005131043726578355 \n",
      "iteration:314 loss:0.0004865036316914484 \n",
      "iteration:315 loss:0.00046099739120109007 \n",
      "iteration:316 loss:0.0004367382025520783 \n",
      "iteration:317 loss:0.0004137460346100852 \n",
      "iteration:318 loss:0.0003918624861398712 \n",
      "iteration:319 loss:0.00037150460775592364 \n",
      "iteration:320 loss:0.0003526656510075554 \n",
      "iteration:321 loss:0.00033432933560106903 \n",
      "iteration:322 loss:0.0003166623901051935 \n",
      "iteration:323 loss:0.0002995483737322502 \n",
      "iteration:324 loss:0.0002836398853105493 \n",
      "iteration:325 loss:0.00026932313267025165 \n",
      "iteration:326 loss:0.0002551824909460265 \n",
      "iteration:327 loss:0.0002417441355646588 \n",
      "iteration:328 loss:0.00022988520140643232 \n",
      "iteration:329 loss:0.00021879599808016792 \n",
      "iteration:330 loss:0.00020815559037146159 \n",
      "iteration:331 loss:0.0001983378897421062 \n",
      "iteration:332 loss:0.00018888811609940603 \n",
      "iteration:333 loss:0.00017962064885068685 \n",
      "iteration:334 loss:0.00017062453844118863 \n",
      "iteration:335 loss:0.00016228055028477684 \n",
      "iteration:336 loss:0.00015420101772178896 \n",
      "iteration:337 loss:0.00014655552513431758 \n",
      "iteration:338 loss:0.00013911024143453687 \n",
      "iteration:339 loss:0.00013137156929587945 \n",
      "iteration:340 loss:0.00012399575643939897 \n",
      "iteration:341 loss:0.00011718007226590998 \n",
      "iteration:342 loss:0.000110817050881451 \n",
      "iteration:343 loss:0.00010484579252079129 \n",
      "iteration:344 loss:9.951438187272288e-05 \n",
      "iteration:345 loss:9.438521738047712e-05 \n",
      "iteration:346 loss:8.966078894445673e-05 \n",
      "iteration:347 loss:8.497036469634622e-05 \n",
      "iteration:348 loss:8.062225970206782e-05 \n",
      "iteration:349 loss:7.662859570700675e-05 \n",
      "iteration:350 loss:7.284245657501742e-05 \n",
      "iteration:351 loss:6.958236554055475e-05 \n",
      "iteration:352 loss:6.637492697336711e-05 \n",
      "iteration:353 loss:6.337073136819527e-05 \n",
      "iteration:354 loss:6.0169513744767755e-05 \n",
      "iteration:355 loss:5.716427040169947e-05 \n",
      "iteration:356 loss:5.4203606850933284e-05 \n",
      "iteration:357 loss:5.138435517437756e-05 \n",
      "iteration:358 loss:4.8536436224821955e-05 \n",
      "iteration:359 loss:4.593851917888969e-05 \n",
      "iteration:360 loss:4.33762397733517e-05 \n",
      "iteration:361 loss:4.1001436329679564e-05 \n",
      "iteration:362 loss:3.874690082739107e-05 \n",
      "iteration:363 loss:3.652851592050865e-05 \n",
      "iteration:364 loss:3.43626961694099e-05 \n",
      "iteration:365 loss:3.222447048756294e-05 \n",
      "iteration:366 loss:3.0329996661748737e-05 \n",
      "iteration:367 loss:2.844542177626863e-05 \n",
      "iteration:368 loss:2.6638241251930594e-05 \n",
      "iteration:369 loss:2.4921777367126197e-05 \n",
      "iteration:370 loss:2.3248849174706265e-05 \n",
      "iteration:371 loss:2.17349297599867e-05 \n",
      "iteration:372 loss:2.0375828171381727e-05 \n",
      "iteration:373 loss:1.915692337206565e-05 \n",
      "iteration:374 loss:1.8062379240291193e-05 \n",
      "iteration:375 loss:1.698499545454979e-05 \n",
      "iteration:376 loss:1.6101304936455563e-05 \n",
      "iteration:377 loss:1.525638799648732e-05 \n",
      "iteration:378 loss:1.4431527233682573e-05 \n",
      "iteration:379 loss:1.3743501767748967e-05 \n",
      "iteration:380 loss:1.3122855307301506e-05 \n",
      "iteration:381 loss:1.2512668035924435e-05 \n",
      "iteration:382 loss:1.1948675819439813e-05 \n",
      "iteration:383 loss:1.1462772818049416e-05 \n",
      "iteration:384 loss:1.1133815860375762e-05 \n",
      "iteration:385 loss:1.0774838301585987e-05 \n",
      "iteration:386 loss:1.0416984878247604e-05 \n",
      "iteration:387 loss:1.0114250471815467e-05 \n",
      "iteration:388 loss:9.804582077777013e-06 \n",
      "iteration:389 loss:9.524894267087802e-06 \n",
      "iteration:390 loss:9.300962119596079e-06 \n",
      "iteration:391 loss:9.105468052439392e-06 \n",
      "iteration:392 loss:8.92052412382327e-06 \n",
      "iteration:393 loss:8.727140084374696e-06 \n",
      "iteration:394 loss:8.562561561120674e-06 \n",
      "iteration:395 loss:8.443163096671924e-06 \n",
      "iteration:396 loss:8.322684152517468e-06 \n",
      "iteration:397 loss:8.262544724857435e-06 \n",
      "iteration:398 loss:8.185808837879449e-06 \n",
      "iteration:399 loss:8.13001679489389e-06 \n",
      "iteration:400 loss:8.058330422500148e-06 \n",
      "iteration:401 loss:7.977974746609107e-06 \n",
      "iteration:402 loss:7.983671821421012e-06 \n",
      "iteration:403 loss:7.983671821421012e-06 \n",
      "iteration:404 loss:7.98005930846557e-06 \n",
      "iteration:405 loss:7.983671821421012e-06 \n",
      "iteration:406 loss:7.983671821421012e-06 \n",
      "iteration:407 loss:7.983671821421012e-06 \n",
      "iteration:408 loss:7.98005930846557e-06 \n",
      "iteration:409 loss:7.983671821421012e-06 \n",
      "iteration:410 loss:7.983671821421012e-06 \n",
      "iteration:411 loss:7.987098797457293e-06 \n",
      "iteration:412 loss:7.978480425663292e-06 \n",
      "iteration:413 loss:7.97224129200913e-06 \n",
      "iteration:414 loss:7.98005930846557e-06 \n",
      "iteration:415 loss:7.983671821421012e-06 \n",
      "iteration:416 loss:7.983671821421012e-06 \n",
      "iteration:417 loss:7.983671821421012e-06 \n",
      "iteration:418 loss:7.98005930846557e-06 \n",
      "iteration:419 loss:7.983671821421012e-06 \n",
      "iteration:420 loss:7.983671821421012e-06 \n",
      "iteration:421 loss:7.983671821421012e-06 \n",
      "iteration:422 loss:7.98005930846557e-06 \n",
      "iteration:423 loss:7.983671821421012e-06 \n",
      "iteration:424 loss:7.983671821421012e-06 \n",
      "iteration:425 loss:7.983671821421012e-06 \n",
      "iteration:426 loss:7.97224129200913e-06 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:427 loss:7.987098797457293e-06 \n",
      "iteration:428 loss:7.978480425663292e-06 \n",
      "iteration:429 loss:7.97224129200913e-06 \n",
      "iteration:430 loss:7.98005930846557e-06 \n",
      "iteration:431 loss:7.983671821421012e-06 \n",
      "iteration:432 loss:7.983671821421012e-06 \n",
      "iteration:433 loss:7.983671821421012e-06 \n",
      "iteration:434 loss:7.98005930846557e-06 \n",
      "iteration:435 loss:7.983671821421012e-06 \n",
      "iteration:436 loss:7.983671821421012e-06 \n",
      "iteration:437 loss:7.983671821421012e-06 \n",
      "iteration:438 loss:7.98005930846557e-06 \n",
      "iteration:439 loss:7.983671821421012e-06 \n",
      "iteration:440 loss:7.983671821421012e-06 \n",
      "iteration:441 loss:7.987098797457293e-06 \n",
      "iteration:442 loss:7.978480425663292e-06 \n",
      "iteration:443 loss:7.97224129200913e-06 \n",
      "iteration:444 loss:7.98005930846557e-06 \n",
      "iteration:445 loss:7.983671821421012e-06 \n",
      "iteration:446 loss:7.983671821421012e-06 \n",
      "iteration:447 loss:7.983671821421012e-06 \n",
      "iteration:448 loss:7.98005930846557e-06 \n",
      "iteration:449 loss:7.983671821421012e-06 \n",
      "iteration:450 loss:7.983671821421012e-06 \n",
      "iteration:451 loss:7.983671821421012e-06 \n",
      "iteration:452 loss:7.98005930846557e-06 \n",
      "iteration:453 loss:7.983671821421012e-06 \n",
      "iteration:454 loss:7.983671821421012e-06 \n",
      "iteration:455 loss:7.983671821421012e-06 \n",
      "iteration:456 loss:7.97224129200913e-06 \n",
      "iteration:457 loss:7.987098797457293e-06 \n",
      "iteration:458 loss:7.978480425663292e-06 \n",
      "iteration:459 loss:7.97224129200913e-06 \n",
      "iteration:460 loss:7.98005930846557e-06 \n",
      "iteration:461 loss:7.983671821421012e-06 \n",
      "iteration:462 loss:7.983671821421012e-06 \n",
      "iteration:463 loss:7.983671821421012e-06 \n",
      "iteration:464 loss:7.98005930846557e-06 \n",
      "iteration:465 loss:7.983671821421012e-06 \n",
      "iteration:466 loss:7.983671821421012e-06 \n",
      "iteration:467 loss:7.983671821421012e-06 \n",
      "iteration:468 loss:7.98005930846557e-06 \n",
      "iteration:469 loss:7.983671821421012e-06 \n",
      "iteration:470 loss:7.983671821421012e-06 \n",
      "iteration:471 loss:7.987098797457293e-06 \n",
      "iteration:472 loss:7.978480425663292e-06 \n",
      "iteration:473 loss:7.97224129200913e-06 \n",
      "iteration:474 loss:7.98005930846557e-06 \n",
      "iteration:475 loss:7.983671821421012e-06 \n",
      "iteration:476 loss:7.983671821421012e-06 \n",
      "iteration:477 loss:7.983671821421012e-06 \n",
      "iteration:478 loss:7.98005930846557e-06 \n",
      "iteration:479 loss:7.983671821421012e-06 \n",
      "iteration:480 loss:7.983671821421012e-06 \n",
      "iteration:481 loss:7.983671821421012e-06 \n",
      "iteration:482 loss:7.98005930846557e-06 \n",
      "iteration:483 loss:7.983671821421012e-06 \n",
      "iteration:484 loss:7.983671821421012e-06 \n",
      "iteration:485 loss:7.983671821421012e-06 \n",
      "iteration:486 loss:7.97224129200913e-06 \n",
      "iteration:487 loss:7.987098797457293e-06 \n",
      "iteration:488 loss:7.978480425663292e-06 \n",
      "iteration:489 loss:7.97224129200913e-06 \n",
      "iteration:490 loss:7.98005930846557e-06 \n",
      "iteration:491 loss:7.983671821421012e-06 \n",
      "iteration:492 loss:7.983671821421012e-06 \n",
      "iteration:493 loss:7.983671821421012e-06 \n",
      "iteration:494 loss:7.98005930846557e-06 \n",
      "iteration:495 loss:7.983671821421012e-06 \n",
      "iteration:496 loss:7.983671821421012e-06 \n",
      "iteration:497 loss:7.983671821421012e-06 \n",
      "iteration:498 loss:7.98005930846557e-06 \n",
      "iteration:499 loss:7.983671821421012e-06 \n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    \n",
    "    for i in range(500):\n",
    "        total_loss=0\n",
    "        for x,y in data:\n",
    "            _,l = sess.run([optimize,loss],feed_dict={X:x,Y:y})\n",
    "            total_loss+=l\n",
    "        \n",
    "        print(\"iteration:{} loss:{} \".format(i,total_loss))\n",
    "    w, b = sess.run([w, b]) \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.999489"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_5:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n97FRW4uj8IgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f29111fd8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X, Y = data.T[0], data.T[1]\n",
    "plt.plot(X, Y, 'bo', label='Real data')\n",
    "plt.plot(X, X * w + b, 'r', label='Predicted data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
